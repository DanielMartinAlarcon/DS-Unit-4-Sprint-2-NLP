{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ASfGeMfI6Kgs"
   },
   "source": [
    "### Use Word2Vec to train your own model on a dataset.\n",
    "\n",
    "1) **Optional** - Find your own dataset of documents to train you model on. You are going to need a lot of data, so it's probably not realistic to scrape data for this assignment given the time constraints that we're working under. Try to find a dataset that has > 5000 documents.\n",
    "\n",
    "- If you can't find a dataset to use try this one: <https://www.kaggle.com/c/quora-question-pairs>\n",
    "\n",
    "2) Clean/Tokenize the documents.\n",
    "\n",
    "3) Vectorize the model using Word2Vec and explore the results using each of the following at least one time:\n",
    "\n",
    "- your_model.wv.most_similar()\n",
    "- your_model.wv.similarity()\n",
    "- your_model.wv.doesn't_match()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wy5lYo4K8wEy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "import modin.pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize # Sentence Tokenizer\n",
    "from nltk.tokenize import word_tokenize # Word Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('quora-question-pairs/train.csv')\n",
    "df = df.drop(columns=['id', 'qid1', 'qid2'])\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    255024\n",
       "1    149263\n",
       "Name: is_duplicate, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.is_duplicate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = df.question1.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2 = df.question2.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(col):\n",
    "    table = str.maketrans(string.punctuation,' '*32)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "    cleaned_listings = []\n",
    "    \n",
    "    for row in col:\n",
    "#         print(row)\n",
    "        # Strip punctuation everywhere,\n",
    "        # replacing it with spaces so that words separated only\n",
    "        # by punctuation don't get smooshed together\n",
    "        no_punctuation = row.translate(table)\n",
    "    #     print(\"No Punctuation:\", no_punctuation)\n",
    "    #     print('------------------------------------------')\n",
    "\n",
    "        # Tokenize by word\n",
    "        tokens = word_tokenize(no_punctuation)\n",
    "    #     print(\"Tokens:\", tokens)\n",
    "    #     print('------------------------------------------')\n",
    "\n",
    "        # Make all words lowercase\n",
    "        lowercase_tokens = [w.lower() for w in tokens]\n",
    "    #     print(\"Lowercase:\", lowercase_tokens)\n",
    "    #     print('------------------------------------------')\n",
    "\n",
    "        # Remove words that aren't alphabetic\n",
    "        alphabetic = [w for w in lowercase_tokens if w.isalpha()]\n",
    "    #     print(\"Alphabetic:\", alphabetic)\n",
    "    #     print('------------------------------------------')\n",
    "\n",
    "        # Remove stopwords\n",
    "        words = [w for w in alphabetic if not w in stop_words]\n",
    "    #     print(\"Cleaned Words:\", words)\n",
    "    #     print(\"--------------------------------\")\n",
    "\n",
    "#         # lemmatize!\n",
    "#         lemmas = [lemmatizer.lemmatize(w) for w in words]\n",
    "        # Append to list\n",
    "        cleaned_listings.append(words)\n",
    "        \n",
    "    return cleaned_listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 712 ms, total: 1min\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q1_clean = cleanup(q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 10s, sys: 686 ms, total: 1min 10s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q2_clean = cleanup(q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['q1_clean'] = q1_clean\n",
    "df['q2_clean'] = q2_clean\n",
    "all_qs = q1_clean + q2_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1_clean</th>\n",
       "      <th>q2_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>[step, step, guide, invest, share, market, india]</td>\n",
       "      <td>[step, step, guide, invest, share, market]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>[story, kohinoor, koh, noor, diamond]</td>\n",
       "      <td>[would, happen, indian, government, stole, koh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>[increase, speed, internet, connection, using,...</td>\n",
       "      <td>[internet, speed, increased, hacking, dns]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>[mentally, lonely, solve]</td>\n",
       "      <td>[find, remainder, math, math, divided]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>[one, dissolve, water, quikly, sugar, salt, me...</td>\n",
       "      <td>[fish, would, survive, salt, water]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely? How can I solve...   \n",
       "4  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0  What is the step by step guide to invest in sh...             0   \n",
       "1  What would happen if the Indian government sto...             0   \n",
       "2  How can Internet speed be increased by hacking...             0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0   \n",
       "4            Which fish would survive in salt water?             0   \n",
       "\n",
       "                                            q1_clean  \\\n",
       "0  [step, step, guide, invest, share, market, india]   \n",
       "1              [story, kohinoor, koh, noor, diamond]   \n",
       "2  [increase, speed, internet, connection, using,...   \n",
       "3                          [mentally, lonely, solve]   \n",
       "4  [one, dissolve, water, quikly, sugar, salt, me...   \n",
       "\n",
       "                                            q2_clean  \n",
       "0         [step, step, guide, invest, share, market]  \n",
       "1  [would, happen, indian, government, stole, koh...  \n",
       "2         [internet, speed, increased, hacking, dns]  \n",
       "3             [find, remainder, math, math, divided]  \n",
       "4                [fish, would, survive, salt, water]  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 53464 words, keeping 11202 word types\n",
      "PROGRESS: at sentence #20000, processed 107171 words, keeping 16008 word types\n",
      "PROGRESS: at sentence #30000, processed 160656 words, keeping 19551 word types\n",
      "PROGRESS: at sentence #40000, processed 213584 words, keeping 22394 word types\n",
      "PROGRESS: at sentence #50000, processed 267253 words, keeping 25055 word types\n",
      "PROGRESS: at sentence #60000, processed 320680 words, keeping 27280 word types\n",
      "PROGRESS: at sentence #70000, processed 373981 words, keeping 29275 word types\n",
      "PROGRESS: at sentence #80000, processed 427468 words, keeping 31006 word types\n",
      "PROGRESS: at sentence #90000, processed 480867 words, keeping 32662 word types\n",
      "PROGRESS: at sentence #100000, processed 534302 words, keeping 34257 word types\n",
      "PROGRESS: at sentence #110000, processed 587129 words, keeping 35737 word types\n",
      "PROGRESS: at sentence #120000, processed 640381 words, keeping 37139 word types\n",
      "PROGRESS: at sentence #130000, processed 693988 words, keeping 38520 word types\n",
      "PROGRESS: at sentence #140000, processed 747481 words, keeping 39776 word types\n",
      "PROGRESS: at sentence #150000, processed 801333 words, keeping 41055 word types\n",
      "PROGRESS: at sentence #160000, processed 855121 words, keeping 42285 word types\n",
      "PROGRESS: at sentence #170000, processed 908790 words, keeping 43458 word types\n",
      "PROGRESS: at sentence #180000, processed 962552 words, keeping 44588 word types\n",
      "PROGRESS: at sentence #190000, processed 1016137 words, keeping 45653 word types\n",
      "PROGRESS: at sentence #200000, processed 1069233 words, keeping 46724 word types\n",
      "PROGRESS: at sentence #210000, processed 1122280 words, keeping 47703 word types\n",
      "PROGRESS: at sentence #220000, processed 1175531 words, keeping 48630 word types\n",
      "PROGRESS: at sentence #230000, processed 1229522 words, keeping 49596 word types\n",
      "PROGRESS: at sentence #240000, processed 1282846 words, keeping 50491 word types\n",
      "PROGRESS: at sentence #250000, processed 1336833 words, keeping 51394 word types\n",
      "PROGRESS: at sentence #260000, processed 1390763 words, keeping 52330 word types\n",
      "PROGRESS: at sentence #270000, processed 1444739 words, keeping 53185 word types\n",
      "PROGRESS: at sentence #280000, processed 1498180 words, keeping 54018 word types\n",
      "PROGRESS: at sentence #290000, processed 1551728 words, keeping 54860 word types\n",
      "PROGRESS: at sentence #300000, processed 1604880 words, keeping 55692 word types\n",
      "PROGRESS: at sentence #310000, processed 1658441 words, keeping 56496 word types\n",
      "PROGRESS: at sentence #320000, processed 1711723 words, keeping 57285 word types\n",
      "PROGRESS: at sentence #330000, processed 1765596 words, keeping 58083 word types\n",
      "PROGRESS: at sentence #340000, processed 1818932 words, keeping 58789 word types\n",
      "PROGRESS: at sentence #350000, processed 1872558 words, keeping 59450 word types\n",
      "PROGRESS: at sentence #360000, processed 1925908 words, keeping 60104 word types\n",
      "PROGRESS: at sentence #370000, processed 1979541 words, keeping 60800 word types\n",
      "PROGRESS: at sentence #380000, processed 2033432 words, keeping 61487 word types\n",
      "PROGRESS: at sentence #390000, processed 2087894 words, keeping 62220 word types\n",
      "PROGRESS: at sentence #400000, processed 2141861 words, keeping 62862 word types\n",
      "PROGRESS: at sentence #410000, processed 2195918 words, keeping 63437 word types\n",
      "PROGRESS: at sentence #420000, processed 2249760 words, keeping 63903 word types\n",
      "PROGRESS: at sentence #430000, processed 2304055 words, keeping 64406 word types\n",
      "PROGRESS: at sentence #440000, processed 2357974 words, keeping 64853 word types\n",
      "PROGRESS: at sentence #450000, processed 2412341 words, keeping 65295 word types\n",
      "PROGRESS: at sentence #460000, processed 2466461 words, keeping 65740 word types\n",
      "PROGRESS: at sentence #470000, processed 2520350 words, keeping 66191 word types\n",
      "PROGRESS: at sentence #480000, processed 2574445 words, keeping 66659 word types\n",
      "PROGRESS: at sentence #490000, processed 2628720 words, keeping 67096 word types\n",
      "PROGRESS: at sentence #500000, processed 2683000 words, keeping 67549 word types\n",
      "PROGRESS: at sentence #510000, processed 2736238 words, keeping 67976 word types\n",
      "PROGRESS: at sentence #520000, processed 2790050 words, keeping 68367 word types\n",
      "PROGRESS: at sentence #530000, processed 2844408 words, keeping 68764 word types\n",
      "PROGRESS: at sentence #540000, processed 2898833 words, keeping 69205 word types\n",
      "PROGRESS: at sentence #550000, processed 2953391 words, keeping 69647 word types\n",
      "PROGRESS: at sentence #560000, processed 3007642 words, keeping 70081 word types\n",
      "PROGRESS: at sentence #570000, processed 3062128 words, keeping 70484 word types\n",
      "PROGRESS: at sentence #580000, processed 3116547 words, keeping 70896 word types\n",
      "PROGRESS: at sentence #590000, processed 3171206 words, keeping 71294 word types\n",
      "PROGRESS: at sentence #600000, processed 3224708 words, keeping 71640 word types\n",
      "PROGRESS: at sentence #610000, processed 3278965 words, keeping 72027 word types\n",
      "PROGRESS: at sentence #620000, processed 3333023 words, keeping 72404 word types\n",
      "PROGRESS: at sentence #630000, processed 3387080 words, keeping 72845 word types\n",
      "PROGRESS: at sentence #640000, processed 3440456 words, keeping 73222 word types\n",
      "PROGRESS: at sentence #650000, processed 3494812 words, keeping 73618 word types\n",
      "PROGRESS: at sentence #660000, processed 3548828 words, keeping 74033 word types\n",
      "PROGRESS: at sentence #670000, processed 3603325 words, keeping 74405 word types\n",
      "PROGRESS: at sentence #680000, processed 3657883 words, keeping 74769 word types\n",
      "PROGRESS: at sentence #690000, processed 3711972 words, keeping 75132 word types\n",
      "PROGRESS: at sentence #700000, processed 3766030 words, keeping 75505 word types\n",
      "PROGRESS: at sentence #710000, processed 3819780 words, keeping 75866 word types\n",
      "PROGRESS: at sentence #720000, processed 3874052 words, keeping 76224 word types\n",
      "PROGRESS: at sentence #730000, processed 3927704 words, keeping 76547 word types\n",
      "PROGRESS: at sentence #740000, processed 3981848 words, keeping 76887 word types\n",
      "PROGRESS: at sentence #750000, processed 4035706 words, keeping 77245 word types\n",
      "PROGRESS: at sentence #760000, processed 4089994 words, keeping 77589 word types\n",
      "PROGRESS: at sentence #770000, processed 4144143 words, keeping 77940 word types\n",
      "PROGRESS: at sentence #780000, processed 4198334 words, keeping 78273 word types\n",
      "PROGRESS: at sentence #790000, processed 4252648 words, keeping 78637 word types\n",
      "PROGRESS: at sentence #800000, processed 4307604 words, keeping 78983 word types\n",
      "collected 79308 word types from a corpus of 4354601 raw words and 808574 sentences\n",
      "Loading a fresh vocabulary\n",
      "min_count=20 retains 13899 unique words (17% of original 79308, drops 65409)\n",
      "min_count=20 leaves 4128701 word corpus (94% of original 4354601, drops 225900)\n",
      "deleting the raw counts dictionary of 79308 items\n",
      "sample=0.001 downsamples 24 most-common words\n",
      "downsampling leaves estimated 3955099 word corpus (95.8% of prior 4128701)\n",
      "estimated required memory for 13899 words and 200 dimensions: 29187900 bytes\n",
      "resetting layer weights\n",
      "training model with 3 workers on 13899 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=3\n",
      "EPOCH 1 - PROGRESS: at 16.21% examples, 628063 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 1 - PROGRESS: at 32.59% examples, 628161 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 1 - PROGRESS: at 49.41% examples, 636921 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 1 - PROGRESS: at 66.33% examples, 644961 words/s, in_qsize 6, out_qsize 0\n",
      "EPOCH 1 - PROGRESS: at 83.66% examples, 654021 words/s, in_qsize 5, out_qsize 0\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 1 : training on 4354601 raw words (3955379 effective words) took 6.0s, 659199 effective words/s\n",
      "EPOCH 2 - PROGRESS: at 16.21% examples, 631192 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 2 - PROGRESS: at 33.27% examples, 645830 words/s, in_qsize 6, out_qsize 0\n",
      "EPOCH 2 - PROGRESS: at 50.10% examples, 647171 words/s, in_qsize 6, out_qsize 0\n",
      "EPOCH 2 - PROGRESS: at 67.69% examples, 658667 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 2 - PROGRESS: at 84.35% examples, 659124 words/s, in_qsize 5, out_qsize 0\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 2 : training on 4354601 raw words (3955310 effective words) took 5.9s, 667051 effective words/s\n",
      "EPOCH 3 - PROGRESS: at 16.44% examples, 642745 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 31.90% examples, 623492 words/s, in_qsize 6, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 47.81% examples, 622586 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 63.60% examples, 622159 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 80.94% examples, 634715 words/s, in_qsize 6, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 98.06% examples, 641670 words/s, in_qsize 5, out_qsize 0\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 3 : training on 4354601 raw words (3954951 effective words) took 6.1s, 643621 effective words/s\n",
      "EPOCH 4 - PROGRESS: at 16.90% examples, 657748 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 4 - PROGRESS: at 34.19% examples, 666716 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 4 - PROGRESS: at 51.47% examples, 669372 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 4 - PROGRESS: at 68.83% examples, 673503 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 4 - PROGRESS: at 85.72% examples, 670413 words/s, in_qsize 6, out_qsize 0\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 4 : training on 4354601 raw words (3955115 effective words) took 5.9s, 675067 effective words/s\n",
      "EPOCH 5 - PROGRESS: at 16.21% examples, 633083 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 5 - PROGRESS: at 32.59% examples, 635303 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 5 - PROGRESS: at 50.10% examples, 651448 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 5 - PROGRESS: at 67.69% examples, 657313 words/s, in_qsize 5, out_qsize 1\n",
      "EPOCH 5 - PROGRESS: at 84.12% examples, 656880 words/s, in_qsize 5, out_qsize 0\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 5 : training on 4354601 raw words (3955088 effective words) took 6.0s, 657095 effective words/s\n",
      "training on a 21773005 raw words (19775843 effective words) took 30.0s, 658464 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 25s, sys: 657 ms, total: 1min 26s\n",
      "Wall time: 33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model1 = Word2Vec(all_qs, min_count=20, window=3, size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['step',\n",
       " 'guide',\n",
       " 'invest',\n",
       " 'share',\n",
       " 'market',\n",
       " 'india',\n",
       " 'story',\n",
       " 'kohinoor',\n",
       " 'koh',\n",
       " 'diamond',\n",
       " 'increase',\n",
       " 'speed',\n",
       " 'internet',\n",
       " 'connection',\n",
       " 'using',\n",
       " 'vpn',\n",
       " 'mentally',\n",
       " 'lonely',\n",
       " 'solve',\n",
       " 'one',\n",
       " 'dissolve',\n",
       " 'water',\n",
       " 'sugar',\n",
       " 'salt',\n",
       " 'methane',\n",
       " 'carbon',\n",
       " 'di',\n",
       " 'oxide',\n",
       " 'astrology',\n",
       " 'capricorn',\n",
       " 'sun',\n",
       " 'cap',\n",
       " 'moon',\n",
       " 'rising',\n",
       " 'say',\n",
       " 'buy',\n",
       " 'good',\n",
       " 'use',\n",
       " 'instead',\n",
       " 'motorola',\n",
       " 'company',\n",
       " 'hack',\n",
       " 'charter',\n",
       " 'method',\n",
       " 'find',\n",
       " 'separation',\n",
       " 'read',\n",
       " 'youtube',\n",
       " 'comments',\n",
       " 'make',\n",
       " 'physics',\n",
       " 'easy',\n",
       " 'learn',\n",
       " 'first',\n",
       " 'sexual',\n",
       " 'experience',\n",
       " 'like',\n",
       " 'laws',\n",
       " 'change',\n",
       " 'status',\n",
       " 'student',\n",
       " 'visa',\n",
       " 'green',\n",
       " 'card',\n",
       " 'us',\n",
       " 'compare',\n",
       " 'immigration',\n",
       " 'canada',\n",
       " 'would',\n",
       " 'trump',\n",
       " 'presidency',\n",
       " 'mean',\n",
       " 'current',\n",
       " 'international',\n",
       " 'master',\n",
       " 'students',\n",
       " 'manipulation',\n",
       " 'girls',\n",
       " 'want',\n",
       " 'friends',\n",
       " 'guy',\n",
       " 'reject',\n",
       " 'many',\n",
       " 'quora',\n",
       " 'users',\n",
       " 'posting',\n",
       " 'questions',\n",
       " 'readily',\n",
       " 'answered',\n",
       " 'google',\n",
       " 'best',\n",
       " 'digital',\n",
       " 'marketing',\n",
       " 'institution',\n",
       " 'banglore',\n",
       " 'rockets',\n",
       " 'look',\n",
       " 'white',\n",
       " 'causing',\n",
       " 'someone',\n",
       " 'jealous',\n",
       " 'ask',\n",
       " 'much',\n",
       " 'hp',\n",
       " 'every',\n",
       " 'time',\n",
       " 'clock',\n",
       " 'numbers',\n",
       " 'tips',\n",
       " 'making',\n",
       " 'job',\n",
       " 'interview',\n",
       " 'process',\n",
       " 'medicines',\n",
       " 'web',\n",
       " 'application',\n",
       " 'society',\n",
       " 'place',\n",
       " 'importance',\n",
       " 'sports',\n",
       " 'way',\n",
       " 'money',\n",
       " 'online',\n",
       " 'prepare',\n",
       " 'ca',\n",
       " 'final',\n",
       " 'law',\n",
       " 'thing',\n",
       " 'better',\n",
       " 'special',\n",
       " 'cares',\n",
       " 'nose',\n",
       " 'gets',\n",
       " 'stuffy',\n",
       " 'night',\n",
       " 'game',\n",
       " 'thrones',\n",
       " 'villain',\n",
       " 'likely',\n",
       " 'give',\n",
       " 'mercy',\n",
       " 'united',\n",
       " 'states',\n",
       " 'government',\n",
       " 'still',\n",
       " 'employment',\n",
       " 'etc',\n",
       " 'citizens',\n",
       " 'political',\n",
       " 'views',\n",
       " 'travel',\n",
       " 'website',\n",
       " 'spain',\n",
       " 'people',\n",
       " 'think',\n",
       " 'obama',\n",
       " 'try',\n",
       " 'take',\n",
       " 'guns',\n",
       " 'away',\n",
       " 'year',\n",
       " 'old',\n",
       " 'improve',\n",
       " 'skills',\n",
       " 'become',\n",
       " 'entrepreneur',\n",
       " 'next',\n",
       " 'years',\n",
       " 'girlfriend',\n",
       " 'asks',\n",
       " 'boyfriend',\n",
       " 'choose',\n",
       " 'makes',\n",
       " 'reply',\n",
       " 'upsc',\n",
       " 'stall',\n",
       " 'f',\n",
       " 'wings',\n",
       " 'fully',\n",
       " 'back',\n",
       " 'squat',\n",
       " 'expect',\n",
       " 'cognizant',\n",
       " 'confirmation',\n",
       " 'mail',\n",
       " 'month',\n",
       " 'day',\n",
       " 'trading',\n",
       " 'kid',\n",
       " 'rebel',\n",
       " 'worth',\n",
       " 'long',\n",
       " 'run',\n",
       " 'universities',\n",
       " 'recruit',\n",
       " 'new',\n",
       " 'grads',\n",
       " 'majors',\n",
       " 'looking',\n",
       " 'quickest',\n",
       " 'instagram',\n",
       " 'followers',\n",
       " 'darth',\n",
       " 'vader',\n",
       " 'fought',\n",
       " 'star',\n",
       " 'wars',\n",
       " 'legends',\n",
       " 'stages',\n",
       " 'breaking',\n",
       " 'couple',\n",
       " 'happens',\n",
       " 'emotionally',\n",
       " 'whether',\n",
       " 'male',\n",
       " 'female',\n",
       " 'examples',\n",
       " 'products',\n",
       " 'crude',\n",
       " 'oil',\n",
       " 'career',\n",
       " 'launcher',\n",
       " 'rbi',\n",
       " 'grade',\n",
       " 'b',\n",
       " 'preparation',\n",
       " 'blu',\n",
       " 'ray',\n",
       " 'play',\n",
       " 'regular',\n",
       " 'dvd',\n",
       " 'player',\n",
       " 'nd',\n",
       " 'always',\n",
       " 'sad',\n",
       " 'memorable',\n",
       " 'ever',\n",
       " 'eaten',\n",
       " 'gst',\n",
       " 'affects',\n",
       " 'tax',\n",
       " 'officers',\n",
       " 'difficult',\n",
       " 'get',\n",
       " 'friend',\n",
       " 'rap',\n",
       " 'songs',\n",
       " 'dance',\n",
       " 'suddenly',\n",
       " 'logged',\n",
       " 'gmail',\n",
       " 'remember',\n",
       " 'password',\n",
       " 'realized',\n",
       " 'recovery',\n",
       " 'email',\n",
       " 'longer',\n",
       " 'alive',\n",
       " 'ways',\n",
       " 'french',\n",
       " 'download',\n",
       " 'content',\n",
       " 'kickass',\n",
       " 'torrent',\n",
       " 'without',\n",
       " 'registration',\n",
       " 'normal',\n",
       " 'dark',\n",
       " 'ring',\n",
       " 'around',\n",
       " 'iris',\n",
       " 'eye',\n",
       " 'harry',\n",
       " 'potter',\n",
       " 'book',\n",
       " 'cursed',\n",
       " 'child',\n",
       " 'depressed',\n",
       " 'european',\n",
       " 'family',\n",
       " 'office',\n",
       " 'database',\n",
       " 'java',\n",
       " 'programming',\n",
       " 'language',\n",
       " 'made',\n",
       " 'store',\n",
       " 'energy',\n",
       " 'produced',\n",
       " 'lightning',\n",
       " 'review',\n",
       " 'performance',\n",
       " 'testing',\n",
       " 'cost',\n",
       " 'privacy',\n",
       " 'germany',\n",
       " 'come',\n",
       " 'else',\n",
       " 'lost',\n",
       " 'gain',\n",
       " 'types',\n",
       " 'immunity',\n",
       " 'narcissistic',\n",
       " 'personality',\n",
       " 'disorder',\n",
       " 'speak',\n",
       " 'english',\n",
       " 'fluently',\n",
       " 'helpful',\n",
       " 'quickbooks',\n",
       " 'auto',\n",
       " 'data',\n",
       " 'support',\n",
       " 'phone',\n",
       " 'number',\n",
       " 'recover',\n",
       " 'corrupted',\n",
       " 'files',\n",
       " 'richest',\n",
       " 'gambler',\n",
       " 'reach',\n",
       " 'level',\n",
       " 'fire',\n",
       " 'bullet',\n",
       " 'backward',\n",
       " 'aircraft',\n",
       " 'going',\n",
       " 'faster',\n",
       " 'backwards',\n",
       " 'prevent',\n",
       " 'breast',\n",
       " 'cancer',\n",
       " 'log',\n",
       " 'account',\n",
       " 'purpose',\n",
       " 'life',\n",
       " 'bjp',\n",
       " 'strip',\n",
       " 'muslims',\n",
       " 'christians',\n",
       " 'indian',\n",
       " 'citizenship',\n",
       " 'put',\n",
       " 'boats',\n",
       " 'burma',\n",
       " 'right',\n",
       " 'etiquette',\n",
       " 'wishing',\n",
       " 'jehovah',\n",
       " 'witness',\n",
       " 'happy',\n",
       " 'birthday',\n",
       " 'wants',\n",
       " 'open',\n",
       " 'commercial',\n",
       " 'fm',\n",
       " 'radio',\n",
       " 'station',\n",
       " 'city',\n",
       " 'procedure',\n",
       " 'swiss',\n",
       " 'despise',\n",
       " 'asians',\n",
       " 'high',\n",
       " 'salary',\n",
       " 'income',\n",
       " 'jobs',\n",
       " 'field',\n",
       " 'biotechnology',\n",
       " 'height',\n",
       " 'also',\n",
       " 'major',\n",
       " 'effects',\n",
       " 'cambodia',\n",
       " 'earthquake',\n",
       " 'kamchatca',\n",
       " 'earthquakes',\n",
       " 'difference',\n",
       " 'fairness',\n",
       " 'gaming',\n",
       " 'laptop',\n",
       " 'inr',\n",
       " 'warrior',\n",
       " 'proving',\n",
       " 'grounds',\n",
       " 'part',\n",
       " 'reference',\n",
       " 'class',\n",
       " 'national',\n",
       " 'institute',\n",
       " 'technology',\n",
       " 'kurukshetra',\n",
       " 'social',\n",
       " 'nitk',\n",
       " 'surathkal',\n",
       " 'romantic',\n",
       " 'movies',\n",
       " 'causes',\n",
       " 'nightmare',\n",
       " 'abstract',\n",
       " 'painting',\n",
       " 'printing',\n",
       " 'work',\n",
       " 'attend',\n",
       " 'caltech',\n",
       " 'jeremy',\n",
       " 'horcrux',\n",
       " 'associate',\n",
       " 'product',\n",
       " 'manager',\n",
       " 'programs',\n",
       " 'early',\n",
       " 'join',\n",
       " 'management',\n",
       " 'rewarding',\n",
       " 'skype',\n",
       " 'busy',\n",
       " 'really',\n",
       " 'war',\n",
       " 'pakistan',\n",
       " 'uri',\n",
       " 'attack',\n",
       " 'ronald',\n",
       " 'reagan',\n",
       " 'speech',\n",
       " 'strategies',\n",
       " 'union',\n",
       " 'civil',\n",
       " 'fiction',\n",
       " 'novel',\n",
       " 'forgot',\n",
       " 'recent',\n",
       " 'demonetisation',\n",
       " 'results',\n",
       " 'higher',\n",
       " 'gdp',\n",
       " 'heard',\n",
       " 'hacking',\n",
       " 'love',\n",
       " 'competitive',\n",
       " 'hiring',\n",
       " 'republic',\n",
       " 'bank',\n",
       " 'helps',\n",
       " 'spam',\n",
       " 'ranking',\n",
       " 'search',\n",
       " 'watch',\n",
       " 'subtitles',\n",
       " 'usa',\n",
       " 'powerful',\n",
       " 'country',\n",
       " 'world',\n",
       " 'obtain',\n",
       " 'instant',\n",
       " 'ulcer',\n",
       " 'pain',\n",
       " 'relief',\n",
       " 'china',\n",
       " 'food',\n",
       " 'taking',\n",
       " 'advantage',\n",
       " 'cry',\n",
       " 'stick',\n",
       " 'tongues',\n",
       " 'pictures',\n",
       " 'ending',\n",
       " 'depressing',\n",
       " 'mind',\n",
       " 'blowing',\n",
       " 'computer',\n",
       " 'tools',\n",
       " 'exist',\n",
       " 'know',\n",
       " 'toothbrush',\n",
       " 'wet',\n",
       " 'dry',\n",
       " 'applying',\n",
       " 'toothpaste',\n",
       " 'question',\n",
       " 'marked',\n",
       " 'needing',\n",
       " 'neutral',\n",
       " 'state',\n",
       " 'buffer',\n",
       " 'mineral',\n",
       " 'holds',\n",
       " 'highest',\n",
       " 'electrical',\n",
       " 'charge',\n",
       " 'greatest',\n",
       " 'mystery',\n",
       " 'universe',\n",
       " 'alternative',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'block',\n",
       " 'sanctions',\n",
       " 'un',\n",
       " 'e',\n",
       " 'mohammad',\n",
       " 'chief',\n",
       " 'masood',\n",
       " 'azhar',\n",
       " 'future',\n",
       " 'budget',\n",
       " 'excessive',\n",
       " 'amounts',\n",
       " 'vitamin',\n",
       " 'c',\n",
       " 'cause',\n",
       " 'miscarriage',\n",
       " 'glass',\n",
       " 'pay',\n",
       " 'scale',\n",
       " 'two',\n",
       " 'access',\n",
       " 'six',\n",
       " 'party',\n",
       " 'talks',\n",
       " 'successful',\n",
       " 'register',\n",
       " 'domain',\n",
       " 'site',\n",
       " 'older',\n",
       " 'men',\n",
       " 'attracted',\n",
       " 'young',\n",
       " 'women',\n",
       " 'strongest',\n",
       " 'structure',\n",
       " 'shape',\n",
       " 'compression',\n",
       " 'matter',\n",
       " 'humans',\n",
       " 'selfish',\n",
       " 'evil',\n",
       " 'active',\n",
       " 'passive',\n",
       " 'transport',\n",
       " 'hair',\n",
       " 'bald',\n",
       " 'head',\n",
       " 'ideal',\n",
       " 'retirement',\n",
       " 'stance',\n",
       " 'websites',\n",
       " 'escorts',\n",
       " 'polo',\n",
       " 'diesel',\n",
       " 'grand',\n",
       " 'petrol',\n",
       " 'black',\n",
       " 'hole',\n",
       " 'mass',\n",
       " 'morgan',\n",
       " 'correct',\n",
       " 'says',\n",
       " 'stop',\n",
       " 'racism',\n",
       " 'talking',\n",
       " 'currently',\n",
       " 'offer',\n",
       " 'employees',\n",
       " 'stock',\n",
       " 'options',\n",
       " 'rsus',\n",
       " 'distribute',\n",
       " 'identical',\n",
       " 'pencils',\n",
       " 'least',\n",
       " 'pencil',\n",
       " 'hire',\n",
       " 'jerry',\n",
       " 'seinfeld',\n",
       " 'hours',\n",
       " 'days',\n",
       " 'late',\n",
       " 'rabies',\n",
       " 'vaccine',\n",
       " 'possible',\n",
       " 'non',\n",
       " 'bite',\n",
       " 'exposure',\n",
       " 'britain',\n",
       " 'ruled',\n",
       " 'afraid',\n",
       " 'working',\n",
       " 'red',\n",
       " 'keep',\n",
       " 'keys',\n",
       " 'age',\n",
       " 'lose',\n",
       " 'virginity',\n",
       " 'could',\n",
       " 'secretly',\n",
       " 'monetize',\n",
       " 'videos',\n",
       " 'upload',\n",
       " 'copyright',\n",
       " 'chances',\n",
       " 'may',\n",
       " 'switch',\n",
       " 'canon',\n",
       " 'members',\n",
       " 'moderation',\n",
       " 'nobody',\n",
       " 'answer',\n",
       " 'funniest',\n",
       " 'joke',\n",
       " 'got',\n",
       " 'london',\n",
       " 'pm',\n",
       " 'deduction',\n",
       " 'pls',\n",
       " 'advice',\n",
       " 'monthly',\n",
       " 'expenses',\n",
       " 'saving',\n",
       " 'stereotypes',\n",
       " 'kingdom',\n",
       " 'twitter',\n",
       " 'business',\n",
       " 'source',\n",
       " 'transfer',\n",
       " 'paypal',\n",
       " 'earphone',\n",
       " 'deep',\n",
       " 'bass',\n",
       " 'eligible',\n",
       " 'yojana',\n",
       " 'forward',\n",
       " 'engineering',\n",
       " 'fields',\n",
       " 'suit',\n",
       " 'abusing',\n",
       " 'gross',\n",
       " 'ctc',\n",
       " 'towns',\n",
       " 'kerala',\n",
       " 'confidence',\n",
       " 'anyone',\n",
       " 'see',\n",
       " 'relation',\n",
       " 'greek',\n",
       " 'gods',\n",
       " 'hindu',\n",
       " 'apple',\n",
       " 'music',\n",
       " 'spotify',\n",
       " 'fixed',\n",
       " 'fund',\n",
       " 'live',\n",
       " 'cologne',\n",
       " 'robert',\n",
       " 'de',\n",
       " 'al',\n",
       " 'prefer',\n",
       " 'small',\n",
       " 'families',\n",
       " 'animals',\n",
       " 'kiss',\n",
       " 'deleted',\n",
       " 'chats',\n",
       " 'addicted',\n",
       " 'hired',\n",
       " 'private',\n",
       " 'eyes',\n",
       " 'ordered',\n",
       " 'follow',\n",
       " 'startup',\n",
       " 'accelerator',\n",
       " 'check',\n",
       " 'wifi',\n",
       " 'history',\n",
       " 'android',\n",
       " 'phones',\n",
       " 'significance',\n",
       " 'battle',\n",
       " 'somme',\n",
       " 'contrast',\n",
       " 'rostov',\n",
       " 'creative',\n",
       " 'college',\n",
       " 'admissions',\n",
       " 'essay',\n",
       " 'happen',\n",
       " 'cover',\n",
       " 'patch',\n",
       " 'pursue',\n",
       " 'different',\n",
       " 'things',\n",
       " 'ben',\n",
       " 'affleck',\n",
       " 'shine',\n",
       " 'christian',\n",
       " 'bale',\n",
       " 'batman',\n",
       " 'start',\n",
       " 'hyderabad',\n",
       " 'possessive',\n",
       " 'wife',\n",
       " 'lord',\n",
       " 'krishna',\n",
       " 'combination',\n",
       " 'courses',\n",
       " 'along',\n",
       " 'enhance',\n",
       " 'screenshot',\n",
       " 'mac',\n",
       " 'executive',\n",
       " 'recruiter',\n",
       " 'psychological',\n",
       " 'need',\n",
       " 'collecting',\n",
       " 'fulfill',\n",
       " 'must',\n",
       " 'tv',\n",
       " 'shows',\n",
       " 'die',\n",
       " 'fluent',\n",
       " 'chinese',\n",
       " 'demonitization',\n",
       " 'rupees',\n",
       " 'notes',\n",
       " 'real',\n",
       " 'estate',\n",
       " 'sector',\n",
       " 'easiest',\n",
       " 'billionaire',\n",
       " 'willing',\n",
       " 'free',\n",
       " 'lectures',\n",
       " 'conduct',\n",
       " 'workshop',\n",
       " 'colleges',\n",
       " 'robotics',\n",
       " 'hate',\n",
       " 'hillary',\n",
       " 'clinton',\n",
       " 'periods',\n",
       " 'regarded',\n",
       " 'prevented',\n",
       " 'rituals',\n",
       " 'hinduism',\n",
       " 'atheism',\n",
       " 'lack',\n",
       " 'belief',\n",
       " 'term',\n",
       " 'god',\n",
       " 'jack',\n",
       " 'quotes',\n",
       " 'lessons',\n",
       " 'assassin',\n",
       " 'creed',\n",
       " 'series',\n",
       " 'printers',\n",
       " 'color',\n",
       " 'ink',\n",
       " 'documents',\n",
       " 'creativity',\n",
       " 'important',\n",
       " 'continue',\n",
       " 'presidential',\n",
       " 'campaign',\n",
       " 'democratic',\n",
       " 'candidate',\n",
       " 'headphones',\n",
       " 'antenna',\n",
       " 'channels',\n",
       " 'mobile',\n",
       " 'companies',\n",
       " 'install',\n",
       " 'inbuilt',\n",
       " 'able',\n",
       " 'growth',\n",
       " 'technologies',\n",
       " 'automation',\n",
       " 'engineers',\n",
       " 'apart',\n",
       " 'disadvantages',\n",
       " 'listing',\n",
       " 'nse',\n",
       " 'object',\n",
       " 'position',\n",
       " 'respect',\n",
       " 'continuous',\n",
       " 'neutron',\n",
       " 'gpa',\n",
       " 'enough',\n",
       " 'top',\n",
       " 'harvard',\n",
       " 'profitable',\n",
       " 'trade',\n",
       " 'binary',\n",
       " 'utilize',\n",
       " 'avoid',\n",
       " 'depression',\n",
       " 'security',\n",
       " 'earn',\n",
       " 'puk',\n",
       " 'code',\n",
       " 'japan',\n",
       " 'sentences',\n",
       " 'word',\n",
       " 'another',\n",
       " 'billion',\n",
       " 'dollar',\n",
       " 'lottery',\n",
       " 'scary',\n",
       " 'drive',\n",
       " 'road',\n",
       " 'hana',\n",
       " 'given',\n",
       " 'turns',\n",
       " 'create',\n",
       " 'shell',\n",
       " 'terminal',\n",
       " 'linux',\n",
       " 'cognition',\n",
       " 'affect',\n",
       " 'perception',\n",
       " 'tqwl',\n",
       " 'ckwl',\n",
       " 'tatkal',\n",
       " 'waiting',\n",
       " 'list',\n",
       " 'generations',\n",
       " 'champions',\n",
       " 'league',\n",
       " 'drinking',\n",
       " 'liters',\n",
       " 'unhealthy',\n",
       " 'great',\n",
       " 'lakes',\n",
       " 'wildlife',\n",
       " 'lake',\n",
       " 'porn',\n",
       " 'stars',\n",
       " 'supporters',\n",
       " 'feel',\n",
       " 'walking',\n",
       " 'promises',\n",
       " 'worried',\n",
       " 'others',\n",
       " 'opinions',\n",
       " 'potty',\n",
       " 'train',\n",
       " 'months',\n",
       " 'pitbull',\n",
       " 'jump',\n",
       " 'rope',\n",
       " 'five',\n",
       " 'minutes',\n",
       " 'calories',\n",
       " 'test',\n",
       " 'gate',\n",
       " 'cs',\n",
       " 'stream',\n",
       " 'material',\n",
       " 'understanding',\n",
       " 'algorithmic',\n",
       " 'analysis',\n",
       " 'newbie',\n",
       " 'temperament',\n",
       " 'husky',\n",
       " 'mix',\n",
       " 'balls',\n",
       " 'weigh',\n",
       " 'weight',\n",
       " 'heavier',\n",
       " 'lighter',\n",
       " 'odd',\n",
       " 'ball',\n",
       " 'weighs',\n",
       " 'walter',\n",
       " 'ad',\n",
       " 'argue',\n",
       " 'signs',\n",
       " 'ultra',\n",
       " 'smart',\n",
       " 'person',\n",
       " 'playing',\n",
       " 'dumb',\n",
       " 'created',\n",
       " 'expansion',\n",
       " 'infinite',\n",
       " 'published',\n",
       " 'bloomberg',\n",
       " 'flash',\n",
       " 'fast',\n",
       " 'legally',\n",
       " 'tangible',\n",
       " 'profits',\n",
       " 'relatively',\n",
       " 'short',\n",
       " 'period',\n",
       " 'implementation',\n",
       " 'bill',\n",
       " 'impact',\n",
       " 'lives',\n",
       " 'common',\n",
       " 'accurately',\n",
       " 'mental',\n",
       " 'illness',\n",
       " 'diagnosed',\n",
       " 'knee',\n",
       " 'girl',\n",
       " 'qualities',\n",
       " 'leader',\n",
       " 'bay',\n",
       " 'gulf',\n",
       " 'modi',\n",
       " 'win',\n",
       " 'franchise',\n",
       " 'medicine',\n",
       " 'swami',\n",
       " 'vivekananda',\n",
       " 'eat',\n",
       " 'veg',\n",
       " 'egg',\n",
       " 'journey',\n",
       " 'cancel',\n",
       " 'tickets',\n",
       " 'charting',\n",
       " 'done',\n",
       " 'exactly',\n",
       " 'core',\n",
       " 'initiative',\n",
       " 'standards',\n",
       " 'pros',\n",
       " 'cons',\n",
       " 'journal',\n",
       " 'publish',\n",
       " 'paper',\n",
       " 'resolutions',\n",
       " 'bad',\n",
       " 'dream',\n",
       " 'marks',\n",
       " 'score',\n",
       " 'aiims',\n",
       " 'rank',\n",
       " 'operating',\n",
       " 'room',\n",
       " 'surgeons',\n",
       " 'equal',\n",
       " 'disagree',\n",
       " 'surgical',\n",
       " 'tie',\n",
       " 'broken',\n",
       " 'knowledge',\n",
       " 'developing',\n",
       " 'apps',\n",
       " 'scratch',\n",
       " 'body',\n",
       " 'rights',\n",
       " 'prisoner',\n",
       " 'gems',\n",
       " 'clash',\n",
       " 'clans',\n",
       " 'lens',\n",
       " 'ex',\n",
       " 'suffering',\n",
       " 'urge',\n",
       " 'visiting',\n",
       " 'placements',\n",
       " 'idea',\n",
       " 'traffic',\n",
       " 'upvotes',\n",
       " 'films',\n",
       " 'feature',\n",
       " 'strong',\n",
       " 'far',\n",
       " 'go',\n",
       " 'wait',\n",
       " 'ir',\n",
       " 'move',\n",
       " 'clothes',\n",
       " 'polyester',\n",
       " 'cotton',\n",
       " 'bone',\n",
       " 'slave',\n",
       " 'name',\n",
       " 'dead',\n",
       " 'float',\n",
       " 'surface',\n",
       " 'drowning',\n",
       " 'talked',\n",
       " 'miss',\n",
       " 'travelling',\n",
       " 'region',\n",
       " 'cities',\n",
       " 'quality',\n",
       " 'customized',\n",
       " 'cupcakes',\n",
       " 'gold',\n",
       " 'coast',\n",
       " 'even',\n",
       " 'plan',\n",
       " 'deal',\n",
       " ...]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = list(model1.wv.vocab)\n",
    "print(len(words))\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fabric', 0.7981115579605103),\n",
       " ('cloth', 0.7871071696281433),\n",
       " ('polyester', 0.7853710651397705),\n",
       " ('fabrics', 0.7839918732643127),\n",
       " ('gin', 0.7832497358322144),\n",
       " ('denim', 0.7773582935333252),\n",
       " ('stain', 0.7771213054656982),\n",
       " ('jacket', 0.7695422172546387),\n",
       " ('wool', 0.7667410373687744),\n",
       " ('grapes', 0.7545533776283264)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.most_similar('cotton', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.018005048766663475"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.similarity('cotton','addicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rabbit'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.doesnt_match(['cotton','rabbit','shirt','polyester'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xkzdOZm38yQ_"
   },
   "source": [
    "### Stretch Goals:\n",
    "\n",
    "1) Use Doc2Vec to train a model on your dataset, and then provide model with a new document and let it find similar documents.\n",
    "\n",
    "2) Download the pre-trained word vectors from Google. Access the pre-trained vectors via the following link: https://code.google.com/archive/p/word2vec\n",
    "\n",
    "Load the pre-trained word vectors and train the Word2vec model\n",
    "\n",
    "Examine the first 100 keys or words of the vocabulary\n",
    "\n",
    "Outputs the vector representation for a select set of words - the words can be of your choice\n",
    "\n",
    "Examine the similarity between words - the words can be of your choice\n",
    "\n",
    "For example:\n",
    "\n",
    "model.similarity('house', 'bungalow')\n",
    "\n",
    "model.similarity('house', 'umbrella')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gakr5rP76IAJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_424_Word_Embeddings_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
