{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QpPcbYew_ttN"
   },
   "source": [
    "# Part 1 - Working with Text Data\n",
    "\n",
    "### Use Python string methods remove irregular whitespace from the following string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "dtotEnsStY5o",
    "outputId": "c4e8a355-5366-4b7a-ddf8-740e9f1fbc8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  This is a    string   that has  \n",
      " a lot of  extra \n",
      "   whitespace.   \n"
     ]
    }
   ],
   "source": [
    "whitespace_string = \"\\n\\n  This is a    string   that has  \\n a lot of  extra \\n   whitespace.   \"\n",
    "\n",
    "print(whitespace_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G9-MkBwasXx8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a string that has a lot of extra whitespace.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(whitespace_string.strip().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vg1-d2aAsXLn"
   },
   "source": [
    "### Use Regular Expressions to take the dates in the following .txt file and put them into a dataframe with columns for:\n",
    "\n",
    "[RegEx dates.txt](https://github.com/ryanleeallred/datasets/blob/master/dates.txt)\n",
    "\n",
    "- Day\n",
    "- Month\n",
    "- Year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KWDiN4C9_0sq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'March 8, 2015\\r\\nMarch 15, 2015\\r\\nMarch 22, 2015\\r\\nMarch 29, 2015\\r\\nApril 5, 2015\\r\\nApril 12, 2015\\r\\nApril 19, 2015\\r\\nApril 26, 2015\\r\\nMay 3, 2015\\r\\nMay 10, 2015\\r\\nMay 17, 2015\\r\\nMay 24, 2015\\r\\nMay 31, 2015\\r\\nJune 7, 2015\\r\\nJune 14, 2015\\r\\nJune 21, 2015\\r\\nJune 28, 2015\\r\\nJuly 5, 2015\\r\\nJuly 12, 2015\\r\\nJuly 19, 2015'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "url = 'https://raw.githubusercontent.com/ryanleeallred/datasets/master/dates.txt'\n",
    "r = requests.get(url)\n",
    "r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>March</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>March</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>March</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>March</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>April</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Day  Month  Year\n",
       "0   8  March  2015\n",
       "1  15  March  2015\n",
       "2  22  March  2015\n",
       "3  29  March  2015\n",
       "4   5  April  2015"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate each line into a string\n",
    "string_dates = r.text.split('\\n')\n",
    "\n",
    "# Strip carriage returns from the string\n",
    "string_dates = [x.strip() for x in string_dates]\n",
    "\n",
    "# Split each string into a list of words\n",
    "list_dates = [re.findall(r'\\w+', row) for row in string_dates]\n",
    "\n",
    "# Extract the elements of that list into a dictionary\n",
    "components = {'Day': [x[1] for x in list_dates],\n",
    "              'Month': [x[0] for x in list_dates],\n",
    "              'Year': [x[2] for x in list_dates]}\n",
    "\n",
    "# Turn the dictionary into a DF\n",
    "df_dates = pd.DataFrame(components)\n",
    "df_dates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4Q0dgoe_uBW"
   },
   "source": [
    "# Part 2 - Bag of Words \n",
    "\n",
    "### Use the twitter sentiment analysis dataset found at this link for the remainder of the Sprint Challenge:\n",
    "\n",
    "[Twitter Sentiment Analysis Dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv)\n",
    "\n",
    " ### Clean and tokenize the documents ensuring the following properties of the text:\n",
    "\n",
    "1) Text should be lowercase.\n",
    "\n",
    "2) Stopwords should be removed.\n",
    "\n",
    "3) Punctuation should be removed.\n",
    "\n",
    "4) Tweets should be tokenized at the word level. \n",
    "\n",
    "(The above don't necessarily need to be completed in that specific order.)\n",
    "\n",
    "### Output some cleaned tweets so that we can see that you made all of the above changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xzdhyTS_3F9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0                       is so sad for my APL frie...\n",
       "1          0                     I missed the New Moon trail...\n",
       "2          1                            omg its already 7:30 :O\n",
       "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv'\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99989"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_feels = df.SentimentText.tolist()\n",
    "len(raw_feels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "punct_table = str.maketrans(string.punctuation,' '*32)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def cleanup(tweet_list):\n",
    "    cleaned_tweets = []\n",
    "\n",
    "    for tweet in tweet_list:\n",
    "\n",
    "        # Strip punctuation everywhere,\n",
    "        # replacing it with spaces so that words separated only\n",
    "        # by punctuation don't get smooshed together\n",
    "        tweet = tweet.translate(punct_table)\n",
    "        \n",
    "        # Tokenize by word\n",
    "        tweet = word_tokenize(tweet)\n",
    "\n",
    "        # Make all words lowercase\n",
    "        tweet = [w.lower() for w in tweet]\n",
    "\n",
    "        # Remove words that aren't alphabetic\n",
    "        tweet = [w for w in tweet if w.isalpha()]\n",
    "\n",
    "        # Remove stopwords\n",
    "        tweet = [w for w in tweet if not w in stop_words]\n",
    "\n",
    "        # Append to list\n",
    "        cleaned_tweets.append(tweet)\n",
    "    \n",
    "    return cleaned_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sad', 'apl', 'friend'],\n",
       " ['missed', 'new', 'moon', 'trailer'],\n",
       " ['omg', 'already'],\n",
       " ['omgaga',\n",
       "  'im',\n",
       "  'sooo',\n",
       "  'im',\n",
       "  'gunna',\n",
       "  'cry',\n",
       "  'dentist',\n",
       "  'since',\n",
       "  'suposed',\n",
       "  'get',\n",
       "  'crown',\n",
       "  'put'],\n",
       " ['think', 'mi', 'bf', 'cheating'],\n",
       " ['worry', 'much'],\n",
       " ['juuuuuuuuuuuuuuuuussssst', 'chillin'],\n",
       " ['sunny', 'work', 'tomorrow', 'tv', 'tonight'],\n",
       " ['handed', 'uniform', 'today', 'miss', 'already'],\n",
       " ['hmmmm', 'wonder', 'number']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feels = cleanup(raw_feels)\n",
    "feels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q764vszGqiUh"
   },
   "source": [
    "### How should TF-IDF scores be interpreted? How are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2Ji7BMhqs3M"
   },
   "source": [
    "TF-IDF scores combine two different measures of a word's importance. They are used in cases where we have a collection of documents, and each document is a collection of words.  Each word in a document is assigned a score.  The score is higher if the word is a high proportion of the words in that document. The score is lower if that word also shows up in a high proportion of all documents.  \n",
    "\n",
    "TF-IDF is designed to match our intuition for how special a word is.  A word will be really special if one document talks about it a lot, and few other documents mention it.\n",
    "\n",
    "The specific equation used to calculate TF-IDF contains one term for each of the measures mentioned, TF (Term Frequency) and IDF (Inverse Document Frequency)\n",
    "\n",
    "`TF(w, d)` = (Number of times word `w` appears in a document) / (Total number of words in the document `d`).\n",
    "\n",
    "`IDF(w)` = log_2(Total number of documents / Number of documents with word `w` in it).\n",
    "\n",
    "`TF-IDF(w, d) = TF(w, d) * IDF(w)`\n",
    "\n",
    "Note that some versions of TF_IDF will use a different base (such as `e`) for the logarithm in IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3iUeBKtG_uEK"
   },
   "source": [
    "# Part 3 - Document Classification\n",
    "\n",
    "1) Use Train_Test_Split to create train and test datasets.\n",
    "\n",
    "2) Vectorize the tokenized documents using your choice of vectorization method. \n",
    "\n",
    " - Stretch goal: Use both of the methods that we talked about in class.\n",
    "\n",
    "3) Create a vocabulary using the X_train dataset and transform both your X_train and X_test data using that vocabulary.\n",
    "\n",
    "4) Use your choice of binary classification algorithm to train and evaluate your model's accuracy. Report both train and test accuracies.\n",
    "\n",
    " - Stretch goal: Use an error metric other than accuracy and implement/evaluate multiple classifiers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the data in the right format\n",
    "X = [' '.join(x) for x in feels]\n",
    "y = df.Sentiment.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vect_and_classify(vect, clf, X, y):\n",
    "    \"\"\"\n",
    "    Split the data, vectorize it, classify it, and evaluate the\n",
    "    classification\n",
    "    \"\"\"\n",
    "    # Split into train and test data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Fit the vectorizer to the training data\n",
    "    vect.fit(X_train)\n",
    "\n",
    "    # Transform traind and test into word vectors\n",
    "    train_vect = vect.transform(X_train)\n",
    "    test_vect = vect.transform(X_test)\n",
    "    \n",
    "    # Fit the classifier, using training data only\n",
    "    clf.fit(train_vect, y_train)\n",
    "    \n",
    "    # Test the classifier's predictions\n",
    "    train_preds = clf.predict(train_vect)\n",
    "    test_preds = clf.predict(test_vect)\n",
    "    \n",
    "    print(f'Train Accuracy: {accuracy_score(y_train, train_preds):.4f}')\n",
    "    print(f'Test Accuracy: {accuracy_score(y_test, test_preds):.4f}')\n",
    "    print()\n",
    "    print(f'Train ROC AUC score: {roc_auc_score(y_train, train_preds):.4f}')\n",
    "    print(f'Test ROC AUC score: {roc_auc_score(y_test, test_preds):.4f}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TX8OEgUP_3ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer & Logistic Regression\n",
      "\n",
      "Train Accuracy: 0.9811\n",
      "Test Accuracy: 0.7508\n",
      "\n",
      "Train ROC AUC score: 0.9797\n",
      "Test ROC AUC score: 0.7400\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(max_features=None, \n",
    "                             ngram_range=(1,2), \n",
    "                             stop_words='english')\n",
    "\n",
    "clf = LogisticRegression(random_state=42)\n",
    "\n",
    "print('Count Vectorizer & Logistic Regression')\n",
    "print()\n",
    "vect_and_classify(vect, clf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorizer & Logistic Regression\n",
      "\n",
      "Train Accuracy: 0.8820\n",
      "Test Accuracy: 0.7470\n",
      "\n",
      "Train ROC AUC score: 0.8736\n",
      "Test ROC AUC score: 0.7361\n"
     ]
    }
   ],
   "source": [
    "vect = TfidfVectorizer(max_features=None, \n",
    "                             ngram_range=(1,2), \n",
    "                             stop_words='english')\n",
    "\n",
    "clf = LogisticRegression(random_state=42)\n",
    "\n",
    "print('TF-IDF Vectorizer & Logistic Regression')\n",
    "print()\n",
    "vect_and_classify(vect, clf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer & LinearSVC Classifier\n",
      "\n",
      "Train Accuracy: 0.9960\n",
      "Test Accuracy: 0.7380\n",
      "\n",
      "Train ROC AUC score: 0.9955\n",
      "Test ROC AUC score: 0.7304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/sandbox/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(max_features=None, \n",
    "                             ngram_range=(1,2), \n",
    "                             stop_words='english')\n",
    "\n",
    "clf = LinearSVC(random_state=42)\n",
    "\n",
    "print('Count Vectorizer & LinearSVC Classifier')\n",
    "print()\n",
    "vect_and_classify(vect, clf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorizer & LinearSVC Classifier\n",
      "\n",
      "Train Accuracy: 0.9946\n",
      "Test Accuracy: 0.7465\n",
      "\n",
      "Train ROC AUC score: 0.9942\n",
      "Test ROC AUC score: 0.7396\n"
     ]
    }
   ],
   "source": [
    "vect = TfidfVectorizer(max_features=None, \n",
    "                             ngram_range=(1,2), \n",
    "                             stop_words='english')\n",
    "\n",
    "clf = LinearSVC(random_state=42)\n",
    "\n",
    "print('TF-IDF Vectorizer & LinearSVC Classifier')\n",
    "print()\n",
    "vect_and_classify(vect, clf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer & Logistic Regression, with an ngram range of (1,1)\n",
      "\n",
      "Train Accuracy: 0.8830\n",
      "Test Accuracy: 0.7453\n",
      "\n",
      "Train ROC AUC score: 0.8768\n",
      "Test ROC AUC score: 0.7351\n"
     ]
    }
   ],
   "source": [
    "# I've been using an ngram-range of (1,2) everywhere.  Just to check, \n",
    "# I wonder whether a range of (1,1) might work better.\n",
    "vect = CountVectorizer(max_features=None, \n",
    "                             ngram_range=(1,1), \n",
    "                             stop_words='english')\n",
    "\n",
    "clf = LogisticRegression(random_state=42)\n",
    "\n",
    "print('Count Vectorizer & Logistic Regression, with an ngram range of (1,1)')\n",
    "print()\n",
    "vect_and_classify(vect, clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sorF95UO_uGx"
   },
   "source": [
    "# Part 4 -  Word2Vec\n",
    "\n",
    "1) Fit a Word2Vec model on your cleaned/tokenized twitter dataset. \n",
    "\n",
    "2) Display the 10 words that are most similar to the word \"twitter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYno4d4N-LHR"
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['sad', 'apl', 'friend'],\n",
       " ['missed', 'new', 'moon', 'trailer'],\n",
       " ['omg', 'already'],\n",
       " ['omgaga',\n",
       "  'im',\n",
       "  'sooo',\n",
       "  'im',\n",
       "  'gunna',\n",
       "  'cry',\n",
       "  'dentist',\n",
       "  'since',\n",
       "  'suposed',\n",
       "  'get',\n",
       "  'crown',\n",
       "  'put'],\n",
       " ['think', 'mi', 'bf', 'cheating']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peek at the input that will go into Word2Vec\n",
    "print(len(feels))\n",
    "feels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 s, sys: 124 ms, total: 17.1 s\n",
      "Wall time: 6.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fit to our cleaned/tokenized data\n",
    "# I tried a few different iterations of the parameters before settling \n",
    "# on these.\n",
    "w2v = Word2Vec(feels, min_count=10, window=3, size=500, negative=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6837"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many words are in our vocabulary?\n",
    "len(list(w2v.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sad',\n",
       " 'friend',\n",
       " 'missed',\n",
       " 'new',\n",
       " 'moon',\n",
       " 'trailer',\n",
       " 'omg',\n",
       " 'already',\n",
       " 'im',\n",
       " 'sooo',\n",
       " 'gunna',\n",
       " 'cry',\n",
       " 'dentist',\n",
       " 'since',\n",
       " 'get',\n",
       " 'crown',\n",
       " 'put',\n",
       " 'think',\n",
       " 'mi',\n",
       " 'bf',\n",
       " 'cheating',\n",
       " 'worry',\n",
       " 'much',\n",
       " 'chillin',\n",
       " 'sunny',\n",
       " 'work',\n",
       " 'tomorrow',\n",
       " 'tv',\n",
       " 'tonight',\n",
       " 'handed',\n",
       " 'uniform',\n",
       " 'today',\n",
       " 'miss',\n",
       " 'hmmmm',\n",
       " 'wonder',\n",
       " 'number',\n",
       " 'must',\n",
       " 'positive',\n",
       " 'thanks',\n",
       " 'haters',\n",
       " 'face',\n",
       " 'day',\n",
       " 'weekend',\n",
       " 'sucked',\n",
       " 'far',\n",
       " 'jb',\n",
       " 'isnt',\n",
       " 'showing',\n",
       " 'australia',\n",
       " 'ok',\n",
       " 'thats',\n",
       " 'win',\n",
       " 'lt',\n",
       " 'way',\n",
       " 'feel',\n",
       " 'right',\n",
       " 'man',\n",
       " 'completely',\n",
       " 'useless',\n",
       " 'rt',\n",
       " 'funny',\n",
       " 'twitter',\n",
       " 'http',\n",
       " 'myloc',\n",
       " 'feeling',\n",
       " 'fine',\n",
       " 'gon',\n",
       " 'na',\n",
       " 'go',\n",
       " 'listen',\n",
       " 'celebrate',\n",
       " 'huge',\n",
       " 'roll',\n",
       " 'thunder',\n",
       " 'scary',\n",
       " 'cut',\n",
       " 'beard',\n",
       " 'growing',\n",
       " 'well',\n",
       " 'year',\n",
       " 'start',\n",
       " 'happy',\n",
       " 'meantime',\n",
       " 'iran',\n",
       " 'one',\n",
       " 'see',\n",
       " 'cause',\n",
       " 'else',\n",
       " 'following',\n",
       " 'pretty',\n",
       " 'awesome',\n",
       " 'level',\n",
       " 'writing',\n",
       " 'massive',\n",
       " 'blog',\n",
       " 'tweet',\n",
       " 'myspace',\n",
       " 'comp',\n",
       " 'shut',\n",
       " 'lost',\n",
       " 'position',\n",
       " 'headed',\n",
       " 'pull',\n",
       " 'golf',\n",
       " 'place',\n",
       " 'something',\n",
       " 'yeah',\n",
       " 'boring',\n",
       " 'whats',\n",
       " 'wrong',\n",
       " 'please',\n",
       " 'tell',\n",
       " 'bothered',\n",
       " 'wish',\n",
       " 'could',\n",
       " 'spend',\n",
       " 'rest',\n",
       " 'life',\n",
       " 'sat',\n",
       " 'going',\n",
       " 'gigs',\n",
       " 'seriously',\n",
       " 'like',\n",
       " 'shit',\n",
       " 'really',\n",
       " 'want',\n",
       " 'sleep',\n",
       " 'nooo',\n",
       " 'hours',\n",
       " 'dancing',\n",
       " 'art',\n",
       " 'assignment',\n",
       " 'finish',\n",
       " 'goodbye',\n",
       " 'exams',\n",
       " 'hello',\n",
       " 'alcohol',\n",
       " 'realize',\n",
       " 'deep',\n",
       " 'geez',\n",
       " 'give',\n",
       " 'girl',\n",
       " 'warning',\n",
       " 'atleast',\n",
       " 'hate',\n",
       " 'appears',\n",
       " 'tear',\n",
       " 'live',\n",
       " 'guys',\n",
       " 'wearing',\n",
       " 'skinny',\n",
       " 'jeans',\n",
       " 'cute',\n",
       " 'heels',\n",
       " 'sure',\n",
       " 'meet',\n",
       " 'meat',\n",
       " 'bit',\n",
       " 'ly',\n",
       " 'moving',\n",
       " 'saturday',\n",
       " 'morning',\n",
       " 'need',\n",
       " 'days',\n",
       " 'week',\n",
       " 'dont',\n",
       " 'room',\n",
       " 'sick',\n",
       " 'cant',\n",
       " 'till',\n",
       " 'walk',\n",
       " 'yay',\n",
       " 'sox',\n",
       " 'great',\n",
       " 'times',\n",
       " 'million',\n",
       " 'uploading',\n",
       " 'pictures',\n",
       " 'type',\n",
       " 'downloads',\n",
       " 'virus',\n",
       " 'brother',\n",
       " 'msn',\n",
       " 'fucked',\n",
       " 'forever',\n",
       " 'amp',\n",
       " 'babes',\n",
       " 'wrote',\n",
       " 'last',\n",
       " 'got',\n",
       " 'call',\n",
       " 'someone',\n",
       " 'york',\n",
       " 'office',\n",
       " 'tumblr',\n",
       " 'com',\n",
       " 'enough',\n",
       " 'said',\n",
       " 'even',\n",
       " 'say',\n",
       " 'anyways',\n",
       " 'chris',\n",
       " 'chicago',\n",
       " 'health',\n",
       " 'class',\n",
       " 'joke',\n",
       " 'show',\n",
       " 'makes',\n",
       " 'look',\n",
       " 'reality',\n",
       " 'time',\n",
       " 'low',\n",
       " 'shall',\n",
       " 'motivation',\n",
       " 'entertainment',\n",
       " 'properly',\n",
       " 'experiment',\n",
       " 'melody',\n",
       " 'another',\n",
       " 'lakers',\n",
       " 'neither',\n",
       " 'magic',\n",
       " 'fun',\n",
       " 'bathroom',\n",
       " 'clean',\n",
       " 'enjoyable',\n",
       " 'tasks',\n",
       " 'boom',\n",
       " 'pow',\n",
       " 'proud',\n",
       " 'congrats',\n",
       " 'though',\n",
       " 'david',\n",
       " 'five',\n",
       " 'end',\n",
       " 'july',\n",
       " 'probably',\n",
       " 'never',\n",
       " 'katie',\n",
       " 'concert',\n",
       " 'friends',\n",
       " 'leaving',\n",
       " 'stupid',\n",
       " 'love',\n",
       " 'ur',\n",
       " 'mom',\n",
       " 'hug',\n",
       " 'harry',\n",
       " 'sunday',\n",
       " 'happiness',\n",
       " 'hand',\n",
       " 'u',\n",
       " 'always',\n",
       " 'backwards',\n",
       " 'cody',\n",
       " 'booo',\n",
       " 'seen',\n",
       " 'allergies',\n",
       " 'hair',\n",
       " 'taking',\n",
       " 'public',\n",
       " 'poll',\n",
       " 'hurts',\n",
       " 'earl',\n",
       " 'jersey',\n",
       " 'first',\n",
       " 'hour',\n",
       " 'sytycd',\n",
       " 'night',\n",
       " 'find',\n",
       " 'online',\n",
       " 'fix',\n",
       " 'thought',\n",
       " 'become',\n",
       " 'second',\n",
       " 'choice',\n",
       " 'may',\n",
       " 'friendly',\n",
       " 'lol',\n",
       " 'plant',\n",
       " 'wan',\n",
       " 'home',\n",
       " 'church',\n",
       " 'wht',\n",
       " 'make',\n",
       " 'pizza',\n",
       " 'inch',\n",
       " 'guitar',\n",
       " 'generous',\n",
       " 'p',\n",
       " 'x',\n",
       " 'miley',\n",
       " 'tour',\n",
       " 'wanted',\n",
       " 'mean',\n",
       " 'kid',\n",
       " 'stick',\n",
       " 'head',\n",
       " 'fly',\n",
       " 'away',\n",
       " 'slow',\n",
       " 'tix',\n",
       " 'send',\n",
       " 'sunshine',\n",
       " 'northern',\n",
       " 'ireland',\n",
       " 'swimming',\n",
       " 'beach',\n",
       " 'would',\n",
       " 'happier',\n",
       " 'walls',\n",
       " 'bedroom',\n",
       " 'painted',\n",
       " 'white',\n",
       " 'idk',\n",
       " 'wat',\n",
       " 'trust',\n",
       " 'sorry',\n",
       " 'da',\n",
       " 'pain',\n",
       " 'caused',\n",
       " 'ima',\n",
       " 'take',\n",
       " 'dis',\n",
       " 'luv',\n",
       " 'yall',\n",
       " 'finding',\n",
       " 'wall',\n",
       " 'math',\n",
       " 'brain',\n",
       " 'heads',\n",
       " 'come',\n",
       " 'save',\n",
       " 'bed',\n",
       " 'cough',\n",
       " 'cab',\n",
       " 'airport',\n",
       " 'christy',\n",
       " 'gt',\n",
       " 'case',\n",
       " 'emo',\n",
       " 'camp',\n",
       " 'wee',\n",
       " 'bringing',\n",
       " 'human',\n",
       " 'rights',\n",
       " 'watch',\n",
       " 'world',\n",
       " 'report',\n",
       " 'hope',\n",
       " 'jonas',\n",
       " 'almost',\n",
       " 'jus',\n",
       " 'fr',\n",
       " 'funeral',\n",
       " 'cried',\n",
       " 'grandpa',\n",
       " 'quot',\n",
       " 'smile',\n",
       " 'cuz',\n",
       " 'graduated',\n",
       " 'longest',\n",
       " 'ever',\n",
       " 'ugh',\n",
       " 'let',\n",
       " 'game',\n",
       " 'grrr',\n",
       " 'says',\n",
       " 'phone',\n",
       " 'immediately',\n",
       " 'mobile',\n",
       " 'paying',\n",
       " 'car',\n",
       " 'stolen',\n",
       " 'mother',\n",
       " 'pose',\n",
       " 'hang',\n",
       " 'girls',\n",
       " 'movie',\n",
       " 'rats',\n",
       " 'plan',\n",
       " 'guess',\n",
       " 'means',\n",
       " 'two',\n",
       " 'cool',\n",
       " 'oh',\n",
       " 'thank',\n",
       " 'pleased',\n",
       " 'guna',\n",
       " 'soon',\n",
       " 'talkin',\n",
       " 'lose',\n",
       " 'rip',\n",
       " 'rose',\n",
       " 'back',\n",
       " 'xmas',\n",
       " 'special',\n",
       " 'damn',\n",
       " 'half',\n",
       " 'aww',\n",
       " 'neighbor',\n",
       " 'status',\n",
       " 'next',\n",
       " 'ex',\n",
       " 'husband',\n",
       " 'daily',\n",
       " 'dose',\n",
       " 'definition',\n",
       " 'entry',\n",
       " 'decided',\n",
       " 'publish',\n",
       " 'help',\n",
       " 'explains',\n",
       " 'alot',\n",
       " 'sequel',\n",
       " 'better',\n",
       " 'fuck',\n",
       " 'told',\n",
       " 'drink',\n",
       " 'w',\n",
       " 'gave',\n",
       " 'b',\n",
       " 'brought',\n",
       " 'neighbors',\n",
       " 'v',\n",
       " 'sweet',\n",
       " 'bought',\n",
       " 'suit',\n",
       " 'true',\n",
       " 'highly',\n",
       " 'actually',\n",
       " 'favorite',\n",
       " 'character',\n",
       " 'book',\n",
       " 'gd',\n",
       " 'knw',\n",
       " 'whyy',\n",
       " 'getting',\n",
       " 'standing',\n",
       " 'pouring',\n",
       " 'rain',\n",
       " 'sitting',\n",
       " 'top',\n",
       " 'summer',\n",
       " 'sucks',\n",
       " 'went',\n",
       " 'dog',\n",
       " 'ear',\n",
       " 'charged',\n",
       " 'us',\n",
       " 'still',\n",
       " 'everytime',\n",
       " 'moves',\n",
       " 'picture',\n",
       " 'naked',\n",
       " 'trains',\n",
       " 'manchester',\n",
       " 'jonasbrothers',\n",
       " 'people',\n",
       " 'picky',\n",
       " 'dice',\n",
       " 'www',\n",
       " 'content',\n",
       " 'wide',\n",
       " 'awake',\n",
       " 'awful',\n",
       " 'weather',\n",
       " 'stinks',\n",
       " 'comin',\n",
       " 'hi',\n",
       " 'stranger',\n",
       " 'frank',\n",
       " 'urself',\n",
       " 'gay',\n",
       " 'asylm',\n",
       " 'panel',\n",
       " 'normal',\n",
       " 'started',\n",
       " 'squarespace',\n",
       " 'bad',\n",
       " 'anything',\n",
       " 'susan',\n",
       " 'boyle',\n",
       " 'didnt',\n",
       " 'diversity',\n",
       " 'good',\n",
       " 'believe',\n",
       " 'goooood',\n",
       " 'graduate',\n",
       " 'paint',\n",
       " 'author',\n",
       " 'lesbian',\n",
       " 'story',\n",
       " 'news',\n",
       " 'unknown',\n",
       " 'error',\n",
       " 'uh',\n",
       " 'iphone',\n",
       " 'os',\n",
       " 'fail',\n",
       " 'giants',\n",
       " 'bs',\n",
       " 'forums',\n",
       " 'php',\n",
       " 'know',\n",
       " 'hungry',\n",
       " 'lets',\n",
       " 'outside',\n",
       " 'balcony',\n",
       " 'eat',\n",
       " 'driving',\n",
       " 'mad',\n",
       " 'men',\n",
       " 'uk',\n",
       " 'female',\n",
       " 'internet',\n",
       " 'starting',\n",
       " 'business',\n",
       " 'women',\n",
       " 'tinyurl',\n",
       " 'truth',\n",
       " 'hiding',\n",
       " 'eyes',\n",
       " 'paramore',\n",
       " 'blip',\n",
       " 'fm',\n",
       " 'try',\n",
       " 'turn',\n",
       " 'inside',\n",
       " 'admit',\n",
       " 'literally',\n",
       " 'coldplay',\n",
       " 'tears',\n",
       " 'kids',\n",
       " 'break',\n",
       " 'dads',\n",
       " 'sighs',\n",
       " 'plurk',\n",
       " 'annoyed',\n",
       " 'via',\n",
       " 'designed',\n",
       " 'gallery',\n",
       " 'nyt',\n",
       " 'ow',\n",
       " 'cries',\n",
       " 'potter',\n",
       " 'comes',\n",
       " 'money',\n",
       " 'mouth',\n",
       " 'cheers',\n",
       " 'sigh',\n",
       " 'stalker',\n",
       " 'divorce',\n",
       " 'move',\n",
       " 'lookin',\n",
       " 'around',\n",
       " 'reason',\n",
       " 'atm',\n",
       " 'little',\n",
       " 'atl',\n",
       " 'ily',\n",
       " 'dies',\n",
       " 'laughter',\n",
       " 'soo',\n",
       " 'watching',\n",
       " 'shopping',\n",
       " 'hopefully',\n",
       " 'best',\n",
       " 'weird',\n",
       " 'many',\n",
       " 'homeless',\n",
       " 'rained',\n",
       " 'truck',\n",
       " 'race',\n",
       " 'worse',\n",
       " 'f',\n",
       " 'confusing',\n",
       " 'hot',\n",
       " 'choco',\n",
       " 'shirt',\n",
       " 'north',\n",
       " 'due',\n",
       " 'body',\n",
       " 'line',\n",
       " 'missing',\n",
       " 'design',\n",
       " 'council',\n",
       " 'tech',\n",
       " 'transfer',\n",
       " 'event',\n",
       " 'everything',\n",
       " 'hard',\n",
       " 'bueno',\n",
       " 'bye',\n",
       " 'ya',\n",
       " 'amazing',\n",
       " 'twits',\n",
       " 'national',\n",
       " 'production',\n",
       " 'priority',\n",
       " 'beats',\n",
       " 'reach',\n",
       " 'enjoy',\n",
       " 'venue',\n",
       " 'shine',\n",
       " 'everywhere',\n",
       " 'everyone',\n",
       " 'train',\n",
       " 'crash',\n",
       " 'dc',\n",
       " 'weeks',\n",
       " 'child',\n",
       " 'nancy',\n",
       " 'tr',\n",
       " 'video',\n",
       " 'wishing',\n",
       " 'ch',\n",
       " 'staying',\n",
       " 'fabulous',\n",
       " 'university',\n",
       " 'happened',\n",
       " 'angels',\n",
       " 'demons',\n",
       " 'lions',\n",
       " 'bears',\n",
       " 'americanwomannn',\n",
       " 'black',\n",
       " 'arm',\n",
       " 'leg',\n",
       " 'wait',\n",
       " 'saw',\n",
       " 'mum',\n",
       " 'fat',\n",
       " 'three',\n",
       " 'external',\n",
       " 'laptop',\n",
       " 'twitpic',\n",
       " 'facebook',\n",
       " 'maybe',\n",
       " 'later',\n",
       " 'c',\n",
       " 'making',\n",
       " 'garlic',\n",
       " 'bread',\n",
       " 'bday',\n",
       " 'pants',\n",
       " 'job',\n",
       " 'fml',\n",
       " 'haha',\n",
       " 'r',\n",
       " 'science',\n",
       " 'cartoons',\n",
       " 'edinburgh',\n",
       " 'fringe',\n",
       " 'btw',\n",
       " 'blame',\n",
       " 'school',\n",
       " 'done',\n",
       " 'short',\n",
       " 'stack',\n",
       " 'thing',\n",
       " 'latter',\n",
       " 'store',\n",
       " 'comics',\n",
       " 'thinking',\n",
       " 'less',\n",
       " 'chance',\n",
       " 'stays',\n",
       " 'orlando',\n",
       " 'wud',\n",
       " 'beautiful',\n",
       " 'servers',\n",
       " 'backup',\n",
       " 'experience',\n",
       " 'problems',\n",
       " 'delay',\n",
       " 'yes',\n",
       " 'j',\n",
       " 'k',\n",
       " 'gentle',\n",
       " 'breeze',\n",
       " 'blown',\n",
       " 'se',\n",
       " 'joseph',\n",
       " 'future',\n",
       " 'nyc',\n",
       " 'cheese',\n",
       " 'burgers',\n",
       " 'nothing',\n",
       " 'kind',\n",
       " 'sort',\n",
       " 'pick',\n",
       " 'flowers',\n",
       " 'teacher',\n",
       " 'jealous',\n",
       " 'without',\n",
       " 'hospital',\n",
       " 'mas',\n",
       " 'security',\n",
       " 'chain',\n",
       " 'strong',\n",
       " 'link',\n",
       " 'youtube',\n",
       " 'account',\n",
       " 'suspended',\n",
       " 'single',\n",
       " 'ladies',\n",
       " 'dance',\n",
       " 'sold',\n",
       " 'hoping',\n",
       " 'nervous',\n",
       " 'aim',\n",
       " 'talk',\n",
       " 'followers',\n",
       " 'grocery',\n",
       " 'sugar',\n",
       " 'ray',\n",
       " 'convo',\n",
       " 'keep',\n",
       " 'pork',\n",
       " 'fridge',\n",
       " 'bitches',\n",
       " 'wake',\n",
       " 'early',\n",
       " 'la',\n",
       " 'opera',\n",
       " 'kay',\n",
       " 'etc',\n",
       " 'bother',\n",
       " 'gum',\n",
       " 'lift',\n",
       " 'surgery',\n",
       " 'tho',\n",
       " 'eek',\n",
       " 'tonite',\n",
       " 'hates',\n",
       " 'app',\n",
       " 'used',\n",
       " 'ka',\n",
       " 'chilling',\n",
       " 'blackberry',\n",
       " 'unlimited',\n",
       " 'every',\n",
       " 'pop',\n",
       " 'quite',\n",
       " 'active',\n",
       " 'shame',\n",
       " 'mia',\n",
       " 'horny',\n",
       " 'island',\n",
       " 'letting',\n",
       " 'check',\n",
       " 'knowledge',\n",
       " 'referring',\n",
       " 'hey',\n",
       " 'thx',\n",
       " 'ttyl',\n",
       " 'sowwy',\n",
       " 'mothers',\n",
       " 'knew',\n",
       " 'written',\n",
       " 'jamie',\n",
       " 'feels',\n",
       " 'bowling',\n",
       " 'ball',\n",
       " 'heart',\n",
       " 'badly',\n",
       " 'keeps',\n",
       " 'crashing',\n",
       " 'load',\n",
       " 'plane',\n",
       " 'current',\n",
       " 'dad',\n",
       " 'mood',\n",
       " 'currently',\n",
       " 'blah',\n",
       " 'parking',\n",
       " 'lot',\n",
       " 'fed',\n",
       " 'matters',\n",
       " 'worst',\n",
       " 'super',\n",
       " 'nasty',\n",
       " 'rainy',\n",
       " 'yuck',\n",
       " 'aiden',\n",
       " 'davis',\n",
       " 'bummed',\n",
       " 'drawing',\n",
       " 'fake',\n",
       " 'stayin',\n",
       " 'lawn',\n",
       " 'tanning',\n",
       " 'play',\n",
       " 'camera',\n",
       " 'settle',\n",
       " 'playing',\n",
       " 'iv',\n",
       " 'drive',\n",
       " 'display',\n",
       " 'upside',\n",
       " 'photos',\n",
       " 'computer',\n",
       " 'effort',\n",
       " 'bring',\n",
       " 'added',\n",
       " 'lease',\n",
       " 'release',\n",
       " 'digital',\n",
       " 'tight',\n",
       " 'response',\n",
       " 'kinda',\n",
       " 'moment',\n",
       " 'silence',\n",
       " 'childhood',\n",
       " 'homes',\n",
       " 'sadly',\n",
       " 'replaced',\n",
       " 'robotpickuplines',\n",
       " 'hilarious',\n",
       " 'punch',\n",
       " 'n',\n",
       " 'crew',\n",
       " 'worth',\n",
       " 'storms',\n",
       " 'loves',\n",
       " 'god',\n",
       " 'friday',\n",
       " 'ti',\n",
       " 'connection',\n",
       " 'died',\n",
       " 'thrilled',\n",
       " 'hearts',\n",
       " 'song',\n",
       " 'apple',\n",
       " 'trackle',\n",
       " 'card',\n",
       " 'helps',\n",
       " 'huh',\n",
       " 'forgot',\n",
       " 'hassle',\n",
       " 'music',\n",
       " 'exhausted',\n",
       " 'ben',\n",
       " 'cake',\n",
       " 'somebody',\n",
       " 'ish',\n",
       " 'long',\n",
       " 'ocean',\n",
       " 'open',\n",
       " 'door',\n",
       " 'throw',\n",
       " 'pc',\n",
       " 'hell',\n",
       " 'bbc',\n",
       " 'made',\n",
       " 'nice',\n",
       " 'twice',\n",
       " 'brilliant',\n",
       " 'xd',\n",
       " 'lonely',\n",
       " 'blue',\n",
       " 'ill',\n",
       " 'fuckin',\n",
       " 'goin',\n",
       " 'portugal',\n",
       " 'drops',\n",
       " 'ye',\n",
       " 'kansas',\n",
       " 'featured',\n",
       " 'html',\n",
       " 'eh',\n",
       " 'leave',\n",
       " 'mexico',\n",
       " 'maths',\n",
       " 'sounds',\n",
       " 'leadership',\n",
       " 'considering',\n",
       " 'library',\n",
       " 'cuts',\n",
       " 'tax',\n",
       " 'sake',\n",
       " 'things',\n",
       " 'town',\n",
       " 'netball',\n",
       " 'match',\n",
       " 'baby',\n",
       " 'member',\n",
       " 'family',\n",
       " 'bedtime',\n",
       " 'cold',\n",
       " 'goes',\n",
       " 'books',\n",
       " 'results',\n",
       " 'poor',\n",
       " 'lil',\n",
       " 'enjoying',\n",
       " 'big',\n",
       " 'rat',\n",
       " 'standard',\n",
       " 'yep',\n",
       " 'epic',\n",
       " 'magazine',\n",
       " 'broke',\n",
       " 'tiger',\n",
       " 'woods',\n",
       " 'wifey',\n",
       " 'sux',\n",
       " 'wondering',\n",
       " 'hay',\n",
       " 'found',\n",
       " 'boy',\n",
       " 'stay',\n",
       " 'master',\n",
       " 'holy',\n",
       " 'iranelection',\n",
       " 'bodies',\n",
       " 'air',\n",
       " 'france',\n",
       " 'twurl',\n",
       " 'nl',\n",
       " 'whole',\n",
       " 'bunch',\n",
       " 'noooooooooo',\n",
       " 'follow',\n",
       " 'hear',\n",
       " 'asking',\n",
       " 'hardest',\n",
       " 'working',\n",
       " 'chica',\n",
       " 'boss',\n",
       " 'lady',\n",
       " 'condition',\n",
       " 'series',\n",
       " 'phoenix',\n",
       " 'boston',\n",
       " 'cleveland',\n",
       " 'teams',\n",
       " 'played',\n",
       " 'terrible',\n",
       " 'soccer',\n",
       " 'basketball',\n",
       " 'desperate',\n",
       " 'bout',\n",
       " 'muah',\n",
       " 'angry',\n",
       " 'praise',\n",
       " 'sry',\n",
       " 'matt',\n",
       " 'couldnt',\n",
       " 'near',\n",
       " 'cell',\n",
       " 'screen',\n",
       " 'dead',\n",
       " 'texts',\n",
       " 'calling',\n",
       " 'sickness',\n",
       " 'strike',\n",
       " 'ohhhh',\n",
       " 'nobody',\n",
       " 'likes',\n",
       " 'reviews',\n",
       " 'read',\n",
       " 'disappointed',\n",
       " 'expecting',\n",
       " 'old',\n",
       " 'gone',\n",
       " 'minutes',\n",
       " 'rings',\n",
       " 'boo',\n",
       " 'dvd',\n",
       " 'suggestion',\n",
       " 'looks',\n",
       " 'awwwww',\n",
       " 'ticket',\n",
       " 'sale',\n",
       " 'monday',\n",
       " 'peace',\n",
       " 'losing',\n",
       " 'pressure',\n",
       " 'bless',\n",
       " 'mic',\n",
       " 'goodnight',\n",
       " 'grandma',\n",
       " 'emergency',\n",
       " 'wonderful',\n",
       " 'g',\n",
       " 'shed',\n",
       " 'cavs',\n",
       " 'soooo',\n",
       " 'point',\n",
       " 'jonathan',\n",
       " 'years',\n",
       " 'passed',\n",
       " 'exam',\n",
       " 'write',\n",
       " 'use',\n",
       " 'value',\n",
       " 'pi',\n",
       " 'rule',\n",
       " ...]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example words from the vocabulary\n",
    "list(w2v.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dm', 0.8473275303840637),\n",
       " ('email', 0.8394845128059387),\n",
       " ('link', 0.8251531720161438),\n",
       " ('sent', 0.8088736534118652),\n",
       " ('facebook', 0.7989238500595093),\n",
       " ('message', 0.7944740056991577),\n",
       " ('info', 0.7845165729522705),\n",
       " ('myspace', 0.7794843912124634),\n",
       " ('list', 0.7783143520355225),\n",
       " ('profile', 0.7758128046989441)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the 10 words that are most similar to the word \"twitter\"\n",
    "w2v.wv.most_similar('twitter', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('luv', 0.7986624240875244),\n",
       " ('sooo', 0.7817818522453308),\n",
       " ('xx', 0.7746195793151855),\n",
       " ('guys', 0.7677934169769287),\n",
       " ('r', 0.7640038728713989),\n",
       " ('doin', 0.7634164094924927),\n",
       " ('boo', 0.7572261691093445),\n",
       " ('goodnight', 0.7514300346374512),\n",
       " ('missed', 0.7511347532272339),\n",
       " ('soooo', 0.7458885312080383)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The main context for the word \"miss\" is people texting\n",
    "# their loved ones.\n",
    "w2v.wv.most_similar(positive=['miss'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('school', 0.5754621028900146),\n",
       " ('bed', 0.5715541839599609),\n",
       " ('sleep', 0.5299521088600159),\n",
       " ('summer', 0.49757999181747437),\n",
       " ('im', 0.48902300000190735),\n",
       " ('home', 0.48606789112091064),\n",
       " ('tomorrow', 0.47004780173301697),\n",
       " ('weekend', 0.46783655881881714),\n",
       " ('tonight', 0.4603281021118164),\n",
       " ('work', 0.4577789306640625)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But if we remove the context of the word \"love\", \"miss\" becomes \n",
    "# instead about times and events that people missed.\n",
    "w2v.wv.most_similar(positive=['miss'], negative=['love'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'matt'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given this random set of words from the vocabulary, we can query\n",
    "# the word \"joe\" and see that it's closest to the word \"matt\" from the list.\n",
    "random_list = ['praise',\n",
    "                 'sry',\n",
    "                 'matt',\n",
    "                 'couldnt',\n",
    "                 'near',\n",
    "                 'cell',\n",
    "                 'screen',\n",
    "                 'dead',\n",
    "                 'texts',\n",
    "                 'calling',\n",
    "                 'sickness',]\n",
    "\n",
    "w2v.wv.most_similar_to_given('joe', random_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DS42SC.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
